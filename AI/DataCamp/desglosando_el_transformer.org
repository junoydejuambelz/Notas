#+TITLE: Desglosando el Transformer
#+AUTHOR: Mou
#+DATE: [2025-08-28]

#+EXPORT_FILE_NAME: desglosando_el_transformer
#+STARTUP: overview

** El artículo que lo cambió todo 
- Attention is all you need (Ashish Vaswani)
  - Mecanismos de atención 
  - Modelado de texto optimizado
  - Usado en los Large Language Models (LLMs)
La arquitectura Transformer está formada por dos bloques, el bloque encoder y el bloque 
decoder

*** Bloque encoder
- Multiples capas idénticas 
- *Lee* y *procesa* el input
- Genera representaciones numéricas ricas en contexto 
- Usa *self-attention* y *feed-forward networks*

*** Bloque decoder 
- Secuencia input codificada $\rightarrow$ secuencia output

*** Positional encoding
- Codifica la posición de cada token en la secuencia 
- El orden es crucial para modelar secuencias

*** Mecanismos de atención 
- Presta atención a los tokens importantes y sus relaciones 
- Mejora la generación de texto 

**** Self-attention
- Le asigna pesos a los tokens 
- Captura dependencias a largo rango 
**** Multi-head attention 
- Parte el input en múltiples *heads*
- Las heads capturas patrones distintos, creando representaciones más ricas
*** Position-wide feed-forward networks 
- Neural Networks (NNs) simples que aplican transformaciones 
- Cada token es transformado de manera independiente
- Independiente de la posición

** Transformers en PyTorch
PyTorch nos provee de una clase de alto nivel en ~torch.nn~ para definir arquitecturas. 
Toma cuatro parámetros principales:

- ~d_model~: Dimensión de los inputs del modelo 
- ~nheads~: Número de attention heads 
- ~num_encoder_layers~: Número de capas del encoder 
- ~num_decoder_layers~: Número de capas del decoder

#+BEGIN_SRC python :session one :results output :exports both
import torch.nn as nn

model = nn.Transformer(
  d_model=512,
  nhead=8,
  num_encoder_layers=6,
  num_decoder_layers=6
)

print(model)
#+END_SRC

#+RESULTS:
#+begin_example
/home/mou/.pyenv/versions/3.11.13/lib/python3.11/site-packages/torch/nn/modules/transformer.py:375: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
#+end_example

