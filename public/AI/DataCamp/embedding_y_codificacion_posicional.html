<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-09-02 Tue 19:43 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Embedding y Codificaci√≥n Posicional</title>
<meta name="generator" content="Org Mode" />
<link rel='stylesheet' href='/style.css' />
<script>
  window.MathJax = {
  tex: {
  inlineMath: [['$', '$'], ['\\(', '\\)']],
  displayMath: [['$$', '$$'], ['\\[', '\\]']],
  processEscapes: true,
  processEnvironments: true,
  packages: {'[+]': ['ams', 'newcommand', 'configmacros', 'action', 'cancel', 'color', 'enclose', 'mhchem', 'unicode', 'verb']}
  },
  loader: {
  load: ['[tex]/newcommand', '[tex]/configmacros', '[tex]/action', '[tex]/cancel', '[tex]/color', '[tex]/enclose', '[tex]/mhchem', '[tex]/unicode', '[tex]/verb']
  },
  options: {
  skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
  };
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
  <script>
  MathJax.startup.defaultReady();
  </script>
</head>
<body>
<div id="preamble" class="status">
<nav class="navbar">
        <div class="navbar-brand">
            <a href="/index.html">üè† Home</a>
        </div>
        <div class="navbar-menu">
                <div class="navbar-item dropdown">
                        <a href="#" class="dropbtn">üñ•Ô∏è Computaci√≥n</a>
                        <div class="dropdown-content">
                                <a href="/Computacion/index.html">üìã Resumen</a>
                                <div class="dropdown-submenu">
                                        <a href="#" class="submenu-btn">‚öôÔ∏è Algoritmos</a>
                                        <div class="submenu-content">
                                                <a href="/Computacion/Algoritmos/index.html">üìã Resumen</a>
                                                <a href="/Computacion/Algoritmos/pensamiento.html">üß† Pensamiento algor√≠tmico</a>
                                        </div>
                                </div>
                        </div>
                </div>
                <div class="navbar-item dropdown">
                        <a href="#" class="dropbtn">ü§ñ AI</a>
                        <div class="dropdown-content">
                                <a href="/AI/index.html">üìã Resumen</a>
                                <div class="dropdown-submenu">
                                        <a href="#" class="submenu-btn">üìö CS229</a>
                                        <div class="submenu-content">
                                                <a href="/AI/CS229/index.html">üìã Resumen</a>
                                                <a href="/AI/CS229/AprendizajeSupervisado/aprendizaje_supervisado.html">üéØ Aprendizaje Supervisado</a>
                                                <a href="/AI/CS229/AprendizajeSupervisado/regresion_lineal.html">üìà Regresi√≥n Lineal</a>
                                        </div>
                                </div>
                                <div class="dropdown-submenu">
                                        <a href="#" class="submenu-btn">üèóÔ∏è DataCamp</a>
                                        <div class="submenu-content">
                                                <a href="/AI/DataCamp/index.html">üìã Resumen</a>
                                                <a href="/AI/DataCamp/desglosando_el_transformer.html">üîç Desglosando el Transformer</a>
                                                <a href="/AI/DataCamp/embedding_y_codificacion_posicional.html">üîó Embedding y Codificaci√≥n Posicional</a>
                                        </div>
                                </div>
                        </div>
                </div>
                <div class="navbar-item dropdown">
                        <a href="#" class="dropbtn">üî¨ HoTT</a>
                        <div class="dropdown-content">
                                <a href="/HoTT/index.html">üìã Resumen</a>
                                <a href="/HoTT/introduccion.html">üöÄ Introducci√≥n</a>
                                <a href="/HoTT/juicios.html">‚öñÔ∏è Juicios</a>
                                <a href="HoTT/transitividad.html">üîõ Transitividad </a>
                        </div>
                </div>
                <div class="navbar-item">
                        <a href="/Emacs/index.html">‚öôÔ∏è Emacs Config</a>
                </div>
        </div>
  </nav>
</div>
<div id="content" class="content">
<h1 class="title">Embedding y Codificaci√≥n Posicional</h1>
<p>
La arquitectura de Transformer empieza embebiendo secuencias como vectores y luego
codificando la posici√≥n de cada token en la secuencia para que los tokens se puedan 
procesar en paralelo.
</p>

<ul class="org-ul">
<li><i>Embedding</i>: Tokens \(\rightarrow\) embedding vector</li>
<li><b>Positional encoding</b>: Token position + embedding vector \(\rightarrow\)
positional encoding</li>
</ul>

<p>
Cada token tiene un ID √∫nico en el vocabulario del modelo, cuando los embebemos usando la 
capa de embedding obtenemos un embedding vector. A la longitud de este vector se le conoce 
como el n√∫mero de dimensiones o dimensionalidad.
</p>
<div id="outline-container-orgb42814b" class="outline-2">
<h2 id="orgb42814b">Embeddings</h2>
<div class="outline-text-2" id="text-orgb42814b">
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">import</span> torch 
<span style="font-weight: bold;">import</span> math 
<span style="font-weight: bold;">import</span> torch.nn <span style="font-weight: bold;">as</span> nn

<span style="font-weight: bold;">class</span> <span style="font-weight: bold; text-decoration: underline;">InputEmbeddings</span>(nn.Module):
  <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">__init__</span>(<span style="font-weight: bold;">self</span>, vocab_size: <span style="font-weight: bold;">int</span>, d_model: <span style="font-weight: bold;">int</span>) -&gt; <span style="font-weight: bold; text-decoration: underline;">None</span>:
    <span style="font-weight: bold;">super</span>().__init__()
    <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">d_model</span> = d_model 
    <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">vocab_size</span> = vocab_size 
    <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">embedding</span> = nn.Embedding(vocab_size, d_model)

  <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">forward</span>(<span style="font-weight: bold;">self</span>, x):
    <span style="font-weight: bold;">return</span> <span style="font-weight: bold;">self</span>.embedding(x) * math.sqrt(<span style="font-weight: bold;">self</span>.d_model)

<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"InputEmbeddings class defined successfully"</span>)
</pre>
</div>

<ul class="org-ul">
<li><b>Pr√°ctica est√°ndar</b>: Escalar por \(\sqrt{d_{model}}\) que asegura que los tokens
token embeddings no sobrecargan o son sobrecargado por los position embeddings.</li>
</ul>
</div>
<div id="outline-container-org3988761" class="outline-3">
<h3 id="org3988761">Creando embeddings</h3>
<div class="outline-text-3" id="text-org3988761">
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">embedding_layer</span> = InputEmbeddings(vocab_size=10_000, d_model=512)
<span style="font-weight: bold; font-style: italic;">embedded_output</span> = embedding_layer(torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]]))
<span style="font-weight: bold;">print</span>(f<span style="font-style: italic;">"Embedded output shape: </span>{embedded_output.shape}<span style="font-style: italic;">"</span>)
<span style="font-weight: bold;">print</span>(f<span style="font-style: italic;">"First few values: </span>{embedded_output[0, 0, :5]}<span style="font-style: italic;">"</span>)
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org2106826" class="outline-2">
<h2 id="org2106826">Positional encoding</h2>
<div class="outline-text-2" id="text-org2106826">
<p>
Codifica la posici√≥n de cada token en la secuencia en un positional embedding y los a√±ade 
a los token embeddings para capturar la posici√≥n. Generalmente estos embeddings tienen las 
mismas dimensiones para facilitar las operaciones. Estos positional embeddings se generan 
utilizando una ecuaci√≥n que usa la posici√≥n del token y las funciones seno y coseno. El 
seno se usa para valores de embedding pares y el coseno para impares. 
</p>

<p>
Para un token en cierta posici√≥n, su positional embedding vector se puede calcular a partir
de estas funciones:
\[ 
  PE_{(pos, 2i)} = \sin\left( \frac{pos}{10000^{2i/d_{model}}} \right) ~~~
  PE_{(pos, 2i + 1)} = \cos\left( \frac{pos}{10000^{2i/d_{model}}} \right)
\]
Donde \(i\) es el n√∫mero de valor.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">class</span> <span style="font-weight: bold; text-decoration: underline;">PositionalEncoding</span>(nn.Module):
  <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">__init__</span>(<span style="font-weight: bold;">self</span>, d_model, max_seq_length):
    <span style="font-weight: bold;">super</span>().__init__()

    <span style="font-weight: bold; font-style: italic;">pe</span> = torch.zeros(max_seq_length, d_model)
    <span style="font-weight: bold; font-style: italic;">position</span> = torch.arange(0, max_seq_length, dtype=torch.<span style="font-weight: bold;">float</span>).unsqueeze(1)
    <span style="font-weight: bold; font-style: italic;">div_term</span> = torch.exp(torch.arange(0, d_model, 2, dtype=torch.<span style="font-weight: bold;">float</span>)) * -(math.log(10000.0) / d_model)

    <span style="font-weight: bold; font-style: italic;">pe</span>[:, 0::2] = torch.sin(position * div_term)
    <span style="font-weight: bold; font-style: italic;">pe</span>[:, 1::2] = torch.cos(position * div_term)

    <span style="font-weight: bold;">self</span>.register_buffer(<span style="font-style: italic;">'pe'</span>, pe.unsqueeze(0))

  <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">forward</span>(<span style="font-weight: bold;">self</span>, x):
    <span style="font-weight: bold;">return</span> x + <span style="font-weight: bold;">self</span>.pe[:, :x.size(1)]  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">Fixed: should be + not *
</span>
<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"PositionalEncoding class defined successfully"</span>)
</pre>
</div>
</div>
<div id="outline-container-org0c10cf9" class="outline-3">
<h3 id="org0c10cf9">Creando positional encodings</h3>
<div class="outline-text-3" id="text-org0c10cf9">
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">pos_encoding_layer</span> = PositionalEncoding(d_model=512, max_seq_length=4)
<span style="font-weight: bold; font-style: italic;">pos_encoded_output</span> = pos_encoding_layer(embedded_output)
<span style="font-weight: bold;">print</span>(f<span style="font-style: italic;">"Positional encoded output shape: </span>{pos_encoded_output.shape}<span style="font-style: italic;">"</span>)
<span style="font-weight: bold;">print</span>(f<span style="font-style: italic;">"Added positional encoding successfully"</span>)
</pre>
</div>
</div>
</div>
</div>
</div>
</body>
</html>
