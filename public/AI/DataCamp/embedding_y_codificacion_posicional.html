<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-09-02 Tue 19:43 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Embedding y Codificación Posicional</title>
<meta name="generator" content="Org Mode" />
<link rel='stylesheet' href='/style.css' />
<script>
  window.MathJax = {
  tex: {
  inlineMath: [['$', '$'], ['\\(', '\\)']],
  displayMath: [['$$', '$$'], ['\\[', '\\]']],
  processEscapes: true,
  processEnvironments: true,
  packages: {'[+]': ['ams', 'newcommand', 'configmacros', 'action', 'cancel', 'color', 'enclose', 'mhchem', 'unicode', 'verb']}
  },
  loader: {
  load: ['[tex]/newcommand', '[tex]/configmacros', '[tex]/action', '[tex]/cancel', '[tex]/color', '[tex]/enclose', '[tex]/mhchem', '[tex]/unicode', '[tex]/verb']
  },
  options: {
  skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
  };
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
  <script>
  MathJax.startup.defaultReady();
  </script>
</head>
<body>
<div id="preamble" class="status">
<nav class="navbar">
        <div class="navbar-brand">
            <a href="/index.html">🏠 Home</a>
        </div>
        <div class="navbar-menu">
                <div class="navbar-item dropdown">
                        <a href="#" class="dropbtn">🖥️ Computación</a>
                        <div class="dropdown-content">
                                <a href="/Computacion/index.html">📋 Resumen</a>
                                <div class="dropdown-submenu">
                                        <a href="#" class="submenu-btn">⚙️ Algoritmos</a>
                                        <div class="submenu-content">
                                                <a href="/Computacion/Algoritmos/index.html">📋 Resumen</a>
                                                <a href="/Computacion/Algoritmos/pensamiento.html">🧠 Pensamiento algorítmico</a>
                                        </div>
                                </div>
                        </div>
                </div>
                <div class="navbar-item dropdown">
                        <a href="#" class="dropbtn">🤖 AI</a>
                        <div class="dropdown-content">
                                <a href="/AI/index.html">📋 Resumen</a>
                                <div class="dropdown-submenu">
                                        <a href="#" class="submenu-btn">📚 CS229</a>
                                        <div class="submenu-content">
                                                <a href="/AI/CS229/index.html">📋 Resumen</a>
                                                <a href="/AI/CS229/AprendizajeSupervisado/aprendizaje_supervisado.html">🎯 Aprendizaje Supervisado</a>
                                                <a href="/AI/CS229/AprendizajeSupervisado/regresion_lineal.html">📈 Regresión Lineal</a>
                                        </div>
                                </div>
                                <div class="dropdown-submenu">
                                        <a href="#" class="submenu-btn">🏗️ DataCamp</a>
                                        <div class="submenu-content">
                                                <a href="/AI/DataCamp/index.html">📋 Resumen</a>
                                                <a href="/AI/DataCamp/desglosando_el_transformer.html">🔍 Desglosando el Transformer</a>
                                                <a href="/AI/DataCamp/embedding_y_codificacion_posicional.html">🔗 Embedding y Codificación Posicional</a>
                                        </div>
                                </div>
                        </div>
                </div>
                <div class="navbar-item dropdown">
                        <a href="#" class="dropbtn">🔬 HoTT</a>
                        <div class="dropdown-content">
                                <a href="/HoTT/index.html">📋 Resumen</a>
                                <a href="/HoTT/introduccion.html">🚀 Introducción</a>
                                <a href="/HoTT/juicios.html">⚖️ Juicios</a>
                                <a href="HoTT/transitividad.html">🔛 Transitividad </a>
                        </div>
                </div>
                <div class="navbar-item">
                        <a href="/Emacs/index.html">⚙️ Emacs Config</a>
                </div>
        </div>
  </nav>
</div>
<div id="content" class="content">
<h1 class="title">Embedding y Codificación Posicional</h1>
<p>
La arquitectura de Transformer empieza embebiendo secuencias como vectores y luego
codificando la posición de cada token en la secuencia para que los tokens se puedan 
procesar en paralelo.
</p>

<ul class="org-ul">
<li><i>Embedding</i>: Tokens \(\rightarrow\) embedding vector</li>
<li><b>Positional encoding</b>: Token position + embedding vector \(\rightarrow\)
positional encoding</li>
</ul>

<p>
Cada token tiene un ID único en el vocabulario del modelo, cuando los embebemos usando la 
capa de embedding obtenemos un embedding vector. A la longitud de este vector se le conoce 
como el número de dimensiones o dimensionalidad.
</p>
<div id="outline-container-orgb42814b" class="outline-2">
<h2 id="orgb42814b">Embeddings</h2>
<div class="outline-text-2" id="text-orgb42814b">
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">import</span> torch 
<span style="font-weight: bold;">import</span> math 
<span style="font-weight: bold;">import</span> torch.nn <span style="font-weight: bold;">as</span> nn

<span style="font-weight: bold;">class</span> <span style="font-weight: bold; text-decoration: underline;">InputEmbeddings</span>(nn.Module):
  <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">__init__</span>(<span style="font-weight: bold;">self</span>, vocab_size: <span style="font-weight: bold;">int</span>, d_model: <span style="font-weight: bold;">int</span>) -&gt; <span style="font-weight: bold; text-decoration: underline;">None</span>:
    <span style="font-weight: bold;">super</span>().__init__()
    <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">d_model</span> = d_model 
    <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">vocab_size</span> = vocab_size 
    <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">embedding</span> = nn.Embedding(vocab_size, d_model)

  <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">forward</span>(<span style="font-weight: bold;">self</span>, x):
    <span style="font-weight: bold;">return</span> <span style="font-weight: bold;">self</span>.embedding(x) * math.sqrt(<span style="font-weight: bold;">self</span>.d_model)

<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"InputEmbeddings class defined successfully"</span>)
</pre>
</div>

<ul class="org-ul">
<li><b>Práctica estándar</b>: Escalar por \(\sqrt{d_{model}}\) que asegura que los tokens
token embeddings no sobrecargan o son sobrecargado por los position embeddings.</li>
</ul>
</div>
<div id="outline-container-org3988761" class="outline-3">
<h3 id="org3988761">Creando embeddings</h3>
<div class="outline-text-3" id="text-org3988761">
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">embedding_layer</span> = InputEmbeddings(vocab_size=10_000, d_model=512)
<span style="font-weight: bold; font-style: italic;">embedded_output</span> = embedding_layer(torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]]))
<span style="font-weight: bold;">print</span>(f<span style="font-style: italic;">"Embedded output shape: </span>{embedded_output.shape}<span style="font-style: italic;">"</span>)
<span style="font-weight: bold;">print</span>(f<span style="font-style: italic;">"First few values: </span>{embedded_output[0, 0, :5]}<span style="font-style: italic;">"</span>)
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org2106826" class="outline-2">
<h2 id="org2106826">Positional encoding</h2>
<div class="outline-text-2" id="text-org2106826">
<p>
Codifica la posición de cada token en la secuencia en un positional embedding y los añade 
a los token embeddings para capturar la posición. Generalmente estos embeddings tienen las 
mismas dimensiones para facilitar las operaciones. Estos positional embeddings se generan 
utilizando una ecuación que usa la posición del token y las funciones seno y coseno. El 
seno se usa para valores de embedding pares y el coseno para impares. 
</p>

<p>
Para un token en cierta posición, su positional embedding vector se puede calcular a partir
de estas funciones:
\[ 
  PE_{(pos, 2i)} = \sin\left( \frac{pos}{10000^{2i/d_{model}}} \right) ~~~
  PE_{(pos, 2i + 1)} = \cos\left( \frac{pos}{10000^{2i/d_{model}}} \right)
\]
Donde \(i\) es el número de valor.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">class</span> <span style="font-weight: bold; text-decoration: underline;">PositionalEncoding</span>(nn.Module):
  <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">__init__</span>(<span style="font-weight: bold;">self</span>, d_model, max_seq_length):
    <span style="font-weight: bold;">super</span>().__init__()

    <span style="font-weight: bold; font-style: italic;">pe</span> = torch.zeros(max_seq_length, d_model)
    <span style="font-weight: bold; font-style: italic;">position</span> = torch.arange(0, max_seq_length, dtype=torch.<span style="font-weight: bold;">float</span>).unsqueeze(1)
    <span style="font-weight: bold; font-style: italic;">div_term</span> = torch.exp(torch.arange(0, d_model, 2, dtype=torch.<span style="font-weight: bold;">float</span>)) * -(math.log(10000.0) / d_model)

    <span style="font-weight: bold; font-style: italic;">pe</span>[:, 0::2] = torch.sin(position * div_term)
    <span style="font-weight: bold; font-style: italic;">pe</span>[:, 1::2] = torch.cos(position * div_term)

    <span style="font-weight: bold;">self</span>.register_buffer(<span style="font-style: italic;">'pe'</span>, pe.unsqueeze(0))

  <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">forward</span>(<span style="font-weight: bold;">self</span>, x):
    <span style="font-weight: bold;">return</span> x + <span style="font-weight: bold;">self</span>.pe[:, :x.size(1)]  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">Fixed: should be + not *
</span>
<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"PositionalEncoding class defined successfully"</span>)
</pre>
</div>
</div>
<div id="outline-container-org0c10cf9" class="outline-3">
<h3 id="org0c10cf9">Creando positional encodings</h3>
<div class="outline-text-3" id="text-org0c10cf9">
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">pos_encoding_layer</span> = PositionalEncoding(d_model=512, max_seq_length=4)
<span style="font-weight: bold; font-style: italic;">pos_encoded_output</span> = pos_encoding_layer(embedded_output)
<span style="font-weight: bold;">print</span>(f<span style="font-style: italic;">"Positional encoded output shape: </span>{pos_encoded_output.shape}<span style="font-style: italic;">"</span>)
<span style="font-weight: bold;">print</span>(f<span style="font-style: italic;">"Added positional encoding successfully"</span>)
</pre>
</div>
</div>
</div>
</div>
</div>
</body>
</html>
