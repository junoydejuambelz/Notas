<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-08-29 Fri 13:00 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Desglosando el Transformer</title>
<meta name="generator" content="Org Mode" />
<link rel='stylesheet' href='/style.css' />
<script>
  window.MathJax = {
  tex: {
  inlineMath: [['$', '$'], ['\\(', '\\)']],
  displayMath: [['$$', '$$'], ['\\[', '\\]']],
  processEscapes: true,
  processEnvironments: true,
  packages: {'[+]': ['ams', 'newcommand', 'configmacros', 'action', 'cancel', 'color', 'enclose', 'mhchem', 'unicode', 'verb']}
  },
  loader: {
  load: ['[tex]/newcommand', '[tex]/configmacros', '[tex]/action', '[tex]/cancel', '[tex]/color', '[tex]/enclose', '[tex]/mhchem', '[tex]/unicode', '[tex]/verb']
  },
  options: {
  skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
  };
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
  <script>
  MathJax.startup.defaultReady();
  </script>
</head>
<body>
<div id="preamble" class="status">
<nav class="navbar">
  <div class="navbar-brand">
  <a href="/index.html">🏠 Home</a>
  </div>
  <div class="navbar-menu">
  <div class="navbar-item dropdown">
  <a href="#" class="dropbtn">🤖 AI</a>
  <div class="dropdown-content">
  <a href="/AI/index.html">📋 Resumen</a>
  <div class="dropdown-submenu">
  <a href="#" class="submenu-btn">📚 CS229</a>
  <div class="submenu-content">
  <a href="/AI/CS229/index.html">📋 Resumen</a>
  <a href="/AI/CS229/AprendizajeSupervisado/aprendizaje_supervisado.html">🎯 Aprendizaje Supervisado</a>
  <a href="/AI/CS229/AprendizajeSupervisado/regresion_lineal.html">📈 Regresión Lineal</a>
  </div>
  </div>
  <div class="dropdown-submenu">
  <a href="#" class="submenu-btn">🏗️ DataCamp</a>
  <div class="submenu-content">
  <a href="/AI/DataCamp/index.html">📋 Resumen</a>
  <a href="/AI/DataCamp/desglosando_el_transformer.html">🔍 Desglosando el Transformer</a>
  <a href="/AI/DataCamp/embedding_y_codificacion_posicional.html">🔗 Embedding y Codificación Posicional</a>
  </div>
  </div>
  </div>
  </div>
  <div class="navbar-item dropdown">
  <a href="#" class="dropbtn">🔬 HoTT</a>
  <div class="dropdown-content">
  <a href="/HoTT/index.html">📋 Resumen</a>
  <a href="/HoTT/introduccion.html">🚀 Introducción</a>
  <a href="/HoTT/juicios.html">⚖️ Juicios</a>
  <a href="HoTT/transitividad.html">🔛 Transitividad </a>
  </div>
  </div>
  <div class="navbar-item">
  <a href="/Emacs/index.html">⚙️ Emacs Config</a>
  </div>
  </div>
  </nav>
</div>
<div id="content" class="content">
<h1 class="title">Desglosando el Transformer</h1>
<div id="outline-container-org2d51ad1" class="outline-2">
<h2 id="org2d51ad1">El artículo que lo cambió todo</h2>
<div class="outline-text-2" id="text-org2d51ad1">
<ul class="org-ul">
<li>Attention is all you need (Ashish Vaswani)
<ul class="org-ul">
<li>Mecanismos de atención</li>
<li>Modelado de texto optimizado</li>
<li>Usado en los Large Language Models (LLMs)</li>
</ul></li>
</ul>
<p>
La arquitectura Transformer está formada por dos bloques, el bloque encoder y el bloque 
decoder
</p>
</div>
<div id="outline-container-orgfa62a27" class="outline-3">
<h3 id="orgfa62a27">Bloque encoder</h3>
<div class="outline-text-3" id="text-orgfa62a27">
<ul class="org-ul">
<li>Multiples capas idénticas</li>
<li><b>Lee</b> y <b>procesa</b> el input</li>
<li>Genera representaciones numéricas ricas en contexto</li>
<li>Usa <b>self-attention</b> y <b>feed-forward networks</b></li>
</ul>
</div>
</div>
<div id="outline-container-org2f9f0ef" class="outline-3">
<h3 id="org2f9f0ef">Bloque decoder</h3>
<div class="outline-text-3" id="text-org2f9f0ef">
<ul class="org-ul">
<li>Secuencia input codificada \(\rightarrow\) secuencia output</li>
</ul>
</div>
</div>
<div id="outline-container-org2267a81" class="outline-3">
<h3 id="org2267a81">Positional encoding</h3>
<div class="outline-text-3" id="text-org2267a81">
<ul class="org-ul">
<li>Codifica la posición de cada token en la secuencia</li>
<li>El orden es crucial para modelar secuencias</li>
</ul>
</div>
</div>
<div id="outline-container-org074534a" class="outline-3">
<h3 id="org074534a">Mecanismos de atención</h3>
<div class="outline-text-3" id="text-org074534a">
<ul class="org-ul">
<li>Presta atención a los tokens importantes y sus relaciones</li>
<li>Mejora la generación de texto</li>
</ul>
</div>
<div id="outline-container-org47d8a98" class="outline-4">
<h4 id="org47d8a98">Self-attention</h4>
<div class="outline-text-4" id="text-org47d8a98">
<ul class="org-ul">
<li>Le asigna pesos a los tokens</li>
<li>Captura dependencias a largo rango</li>
</ul>
</div>
</div>
<div id="outline-container-orgce1ee97" class="outline-4">
<h4 id="orgce1ee97">Multi-head attention</h4>
<div class="outline-text-4" id="text-orgce1ee97">
<ul class="org-ul">
<li>Parte el input en múltiples <b>heads</b></li>
<li>Las heads capturas patrones distintos, creando representaciones más ricas</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org27644bd" class="outline-3">
<h3 id="org27644bd">Position-wide feed-forward networks</h3>
<div class="outline-text-3" id="text-org27644bd">
<ul class="org-ul">
<li>Neural Networks (NNs) simples que aplican transformaciones</li>
<li>Cada token es transformado de manera independiente</li>
<li>Independiente de la posición</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org63fb9f6" class="outline-2">
<h2 id="org63fb9f6">Transformers en PyTorch</h2>
<div class="outline-text-2" id="text-org63fb9f6">
<p>
PyTorch nos provee de una clase de alto nivel en <code>torch.nn</code> para definir arquitecturas. 
Toma cuatro parámetros principales:
</p>

<ul class="org-ul">
<li><code>d_model</code>: Dimensión de los inputs del modelo</li>
<li><code>nheads</code>: Número de attention heads</li>
<li><code>num_encoder_layers</code>: Número de capas del encoder</li>
<li><code>num_decoder_layers</code>: Número de capas del decoder</li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">import</span> torch.nn <span style="font-weight: bold;">as</span> nn

<span style="font-weight: bold; font-style: italic;">model</span> = nn.Transformer(
  d_model=512,
  nhead=8,
  num_encoder_layers=6,
  num_decoder_layers=6
)

<span style="font-weight: bold;">print</span>(model)
</pre>
</div>

<pre class="example" id="orge0f9350">
/home/mou/.pyenv/versions/3.11.13/lib/python3.11/site-packages/torch/nn/modules/transformer.py:375: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
</pre>
</div>
</div>
</div>
</body>
</html>
