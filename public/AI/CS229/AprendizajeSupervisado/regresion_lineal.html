<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-08-28 Thu 10:11 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Regresi√≥n lineal</title>
<meta name="author" content="Mou" />
<meta name="generator" content="Org Mode" />
<link rel='stylesheet' href='./style.css' />
<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div id="preamble" class="status">
<nav class="navbar">
  <div class="navbar-brand">
    <a href="/index.html">üè† Home</a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-item dropdown">
      <a href="#" class="dropbtn">ü§ñ AI</a>
      <div class="dropdown-content">
        <a href="/AI/index.html">üìã Resumen</a>
        <div class="dropdown-submenu">
          <a href="#" class="submenu-btn">üìö CS229 ‚ñ∂</a>
          <div class="submenu-content">
            <a href="/AI/CS229/index.html">üìã Resumen</a>
            <a href="/AI/CS229/AprendizajeSupervisado/aprendizaje_supervisado.html">üéØ Aprendizaje Supervisado</a>
            <a href="/AI/CS229/AprendizajeSupervisado/regresion_lineal.html">üìà Regresi√≥n Lineal</a>
          </div>
        </div>
        <div class="dropdown-submenu">
          <a href="#" class="submenu-btn">üèóÔ∏è DataCamp ‚ñ∂</a>
          <div class="submenu-content">
            <a href="/AI/DataCamp/index.html">üìã Resumen</a>
            <a href="/AI/DataCamp/desglosando_el_transformer.html">üîç Desglosando el Transformer</a>
            <a href="/AI/DataCamp/embedding_y_codificacion_posicional.html">üîó Embedding y Codificaci√≥n Posicional</a>
          </div>
        </div>
      </div>
    </div>
    <div class="navbar-item dropdown">
      <a href="#" class="dropbtn">üî¨ HoTT</a>
      <div class="dropdown-content">
        <a href="/HoTT/index.html">üìã Resumen</a>
        <a href="/HoTT/introduccion.html">üöÄ Introducci√≥n</a>
        <a href="/HoTT/juicios.html">‚öñÔ∏è Juicios</a>
      </div>
    </div>
  </div>
</nav>
</div>
<div id="content" class="content">
<h1 class="title">Regresi√≥n lineal</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org0d7fe4f">1. El algoritmo LMS</a></li>
</ul>
</div>
</div>
<p>
Para hacer m√°s interesante nuestro ejemplo de las casas, consideremos un conjunto de datos
m√°s rico, en el que tambi√©n conocemos el n√∫mero de cuartos en cada casa:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-right">√Årea \((pies^2)\)</td>
<td class="org-right">#cuartos</td>
<td class="org-right">Precio (1000$)</td>
</tr>

<tr>
<td class="org-right">2104</td>
<td class="org-right">3</td>
<td class="org-right">400</td>
</tr>

<tr>
<td class="org-right">1600</td>
<td class="org-right">3</td>
<td class="org-right">330</td>
</tr>

<tr>
<td class="org-right">2400</td>
<td class="org-right">3</td>
<td class="org-right">369</td>
</tr>

<tr>
<td class="org-right">1416</td>
<td class="org-right">2</td>
<td class="org-right">232</td>
</tr>

<tr>
<td class="org-right">3000</td>
<td class="org-right">4</td>
<td class="org-right">540</td>
</tr>

<tr>
<td class="org-right">\(\vdots\)</td>
<td class="org-right">\(\vdots\)</td>
<td class="org-right">\(\vdots\)</td>
</tr>
</tbody>
</table>

<p>
Aqu√≠, las \(x\)'s son vectores de dos dimensiones en \(\mathbb{R}^2\). Por ejemplo, \(x_1^{(i)}\)
es el √°rea de la $i$-√©sima casa en el conjunto de entrenamiento y \(x_2^{(i)}\) el n√∫mero 
de cuartos.
</p>

<p>
Para realizar el aprendizaje supervisado, debemos decidir c√≥mo vamos a representar las 
funciones/hip√≥tesis \(h\) en una computadora. Una elecci√≥n inicial, digamos donde 
aproximamos a \(y\) como una funci√≥n lineal de \(x\):
\[h_\theta = \theta_0 + \theta_1 x + \theta_2 x_2.\]
</p>

<p>
Aqu√≠, las \(\theta_i\)'s son los <b>par√°metros</b> (tambi√©n llamados <b>pesos</b>) que parametrizan
el espacio de funciones lineal que mapean a \(\mathcal{X}\) en \(\mathcal{Y}\).
Cuando no haya peligro de confusi√≥n, dejamos de lado el sub-√≠ndice \(\theta\) y escribimos 
simplemente \(h(x)\). Para simplificar la notaci√≥n, tomamos la convenci√≥n de hacer \(x_0 = 1\)
(este es el t√©rmino de <b>intercepto</b>), de modo que 
\[h(x) = \sum_{i=0}^{d} \theta_i x_i = \theta^T x, \]
donde en el lado derecho estamos viendo a \(\theta\) y a \(x\) como vectores y donde \(d\) es el
n√∫mero de variables de entrada (caracter√≠sticas) sin contar a \(x_0\).
</p>

<p>
Ahora, dado un conjunto de entrenamiento, ¬øc√≥mo escogemos (aprendemos) los par√°metros \(\theta\)?
Un m√©todo razonable ser√≠a hacer a \(h(x)\) cercano a \(y\), al menos para los ejemplos de entrenamiento
que tenemos. Para formalizar esto, definiremos una funci√≥n que mide, para cada valor de las 
\(\theta\)'s, qu√© tan cercanas est√°n las \(h(x^{(i)})\)'s a las \(y^{(i)}\)'s correspondientes. 
Definimos la <b>funci√≥n de costo</b>:
\[ J(\theta) = \frac{1}{2} \sum_{i = 1}^n \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2. \]
A esta funci√≥n de costo se le conoce como la funci√≥n de costo de los m√≠nimos cuadrados y da lugar 
al modelo de regresi√≥n usual de <b>m√≠nimos cuadrados</b>.
</p>
<div id="outline-container-org0d7fe4f" class="outline-2">
<h2 id="org0d7fe4f"><span class="section-number-2">1.</span> El algoritmo LMS</h2>
<div class="outline-text-2" id="text-1">
<p>
Queremos escoger a \(\theta\) de tal manera que se minimice \(J(\theta)\). Para hacer esto, usaremos 
un algoritmo de b√∫squeda que empieza con una suposici√≥n inicial para \(\theta\) y luego cambia 
repetidamente su valor para hacer a \(J(\theta)\) cada vez m√°s peque√±a. Espec√≠ficamente, consideremos
el algoritmo del <b>descenso gradiente</b>, que comienza con una suposici√≥n inicial \(\theta\), y 
aplica repetidamente la siguiente actualizaci√≥n:
\[\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)\]
</p>

<p>
Aqu√≠, \(\alpha\) es llamada la <b>tasa de aprendizaje</b>. Este es un algoritmo muy natural que toma 
repetidamente un paso en la direcci√≥n de m√°ximo decrecimiento de \(J\).
</p>

<p>
Para implementar el algoritmo, debemos calcular cu√°l es el t√©rmino de derivada parcial en el lado 
derecho. Primero hagamos el caso en el que tenemos un solo ejemplo de entrenamiento \((x,y)\), de 
modo que podemos ignorar la definici√≥n de \(J\) como una suma. Tenemos:
</p>

\begin{align*}
  \frac{\partial}{\partial \theta_j} J(\theta) &= \frac{\partial}{\partial \theta_j} \frac{1}{2} (h_\theta)(x) - y)^2 \\
  &= 2 \cdot \frac{1}{2} (h_\theta(x) - y) \cdot \frac{\partial}{\partial \theta_j} (h_\theta(x)-y)\\
  &= (h_\theta(x) - y) \cdot \frac{\partial}{\partial \theta_j} \left( \sum_{i=0}^d \theta_i x_i - y \right) \\
  &= (h_\theta(x) - y)x_j
\end{align*}

<p>
De modo que, para un s√≥lo ejemplo de entrenamiento, esto nos da la regla de actualizaci√≥n:
\[\theta_j := \theta_j + \alpha (y^{(i))} - h_\theta (x^{(i))})) x_j^{(i)}.\]
A esta regla se le conoce como la regla de actualizaci√≥n <b>LMS</b> (Least Mean Squares) o la regla 
de aprendizaje <b>Widrow-Hoff</b>. Esta regla tiene varias propiedades que parecen naturales e
intuitivas. Por ejemplo, la magnitud de la actualizaci√≥n es proporcional al t√©rmino de 
<b>error</b> \((y^{(i))} - h_\theta(x^{(i)})\); por lo tanto, si encontramos un ejemplo de 
entrenamiento en el que nuestra predicci√≥n es casi exacta, el par√°metro se actualiza por 
una cantidad peque√±a y, por el contrario, si el error es grande, se actualiza por una 
cantidad mayor.
</p>
</div>
</div>
</div>
</body>
</html>
