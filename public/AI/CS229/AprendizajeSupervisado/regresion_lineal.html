<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-09-22 Mon 08:14 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Regresi√≥n lineal</title>
<meta name="generator" content="Org Mode" />
<link rel='stylesheet' href='/style.css' />
<script defer src='/navbar.js'></script>
<script>
  window.MathJax = {
  tex: {
  inlineMath: [['$', '$'], ['\\(', '\\)']],
  displayMath: [['$$', '$$'], ['\\[', '\\]']],
  processEscapes: true,
  processEnvironments: true,
  packages: {'[+]': ['ams', 'newcommand', 'configmacros', 'action', 'cancel', 'color', 'enclose', 'mhchem', 'unicode', 'verb']}
  },
  loader: {
  load: ['[tex]/newcommand', '[tex]/configmacros', '[tex]/action', '[tex]/cancel', '[tex]/color', '[tex]/enclose', '[tex]/mhchem', '[tex]/unicode', '[tex]/verb']
  },
  options: {
  skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
  };
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
  <script>
  MathJax.startup.defaultReady();
  </script>
</head>
<body>
<div id="preamble" class="status">
<nav class="navbar">
        <div class="navbar-brand">
            <a href="/index.html">üè† Home</a>
        </div>
        <div class="navbar-menu">
                <div class="navbar-item dropdown">
                        <a href="#" class="dropbtn">üñ•Ô∏è Computaci√≥n</a>
                        <div class="dropdown-content">
                                <a href="/Computacion/index.html">üìã Resumen</a>
                                <div class="dropdown-submenu">
                                        <a href="#" class="submenu-btn">‚öôÔ∏è Algoritmos</a>
                                        <div class="submenu-content">
                                                <a href="/Computacion/Algoritmos/index.html">üìã Resumen</a>
                                                <a href="/Computacion/Algoritmos/pensamiento.html">üß† Pensamiento algor√≠tmico</a>
                                        </div>
                                </div>
                        </div>
                </div>
                <div class="navbar-item dropdown">
                        <a href="#" class="dropbtn">ü§ñ AI</a>
                        <div class="dropdown-content">
                                <a href="/AI/index.html">üìã Resumen</a>
                                <div class="dropdown-submenu">
                                        <a href="#" class="submenu-btn">üìö CS229</a>
                                        <div class="submenu-content">
                                                <a href="/AI/CS229/index.html">üìã Resumen</a>
                                                <a href="/AI/CS229/AprendizajeSupervisado/aprendizaje_supervisado.html">üéØ Aprendizaje Supervisado</a>
                                                <a href="/AI/CS229/AprendizajeSupervisado/regresion_lineal.html">üìà Regresi√≥n Lineal</a>
                                        </div>
                                </div>
                                <div class="dropdown-submenu">
                                        <a href="#" class="submenu-btn">üèóÔ∏è DataCamp</a>
                                        <div class="submenu-content">
                                                <a href="/AI/DataCamp/index.html">üìã Resumen</a>
                                                <a href="/AI/DataCamp/desglosando_el_transformer.html">üîç Desglosando el Transformer</a>
                                                <a href="/AI/DataCamp/embedding_y_codificacion_posicional.html">üîó Embedding y Codificaci√≥n Posicional</a>
                                        </div>
                                </div>
                        </div>
                </div>
                <div class="navbar-item dropdown">
                        <a href="#" class="dropbtn">üî§ Tipos</a>
                        <div class="dropdown-content">

                                <div class="dropdown-submenu">
                                        <a href="#" class="submenu-btn">‚úçÔ∏è Pruebas y tipos</a>
                                        <div class="submenu-content">
                                                <a href="/Tipos/ProofsAndTypes/index.html">üìã Resumen </a>
                                                <a href="/Tipos/ProofsAndTypes/sentido.html">üß† Sentido, denotaci√≥n y sem√°ntica</a>
                                        </div>
                                </div>

                                <div class="dropdown-submenu">
                                        <a href="#" class="submenu-btn">üî¨ HoTT</a>
                                        <div class="submenu-content">
                                                <div class="dropdown-submenu">
                                                        <a href="#" class="submenu-btn">üè´ HoTT (Carnegie Mellon)</a>
                                                        <div class="submenu-content">
                                                                <a href="/Tipos/HoTT/Curso/index.html">üìã Resumen</a>
                                                                <a href="/Tipos/HoTT/Curso/introduccion.html">üöÄ Introducci√≥n</a>
                                                                <a href="/Tipos/HoTT/Curso/juicios.html">‚öñÔ∏è Juicios</a>
                                                                <a href="/Tipos/HoTT/Curso/transitividad.html">üîõ Transitividad </a>
                                                                <a href="/Tipos/HoTT/Curso/exponenciales.html">üóº Exponenciales</a>
                                                                <a href="/Tipos/HoTT/Curso/igualdad.html">üü∞ Igualdad</a>
                                                        </div>
                                                </div>

                                                <div class="dropdown-submenu">
                                                        <a href="#" class="submenu-btn">üìö Introducci√≥n a HoTT (Rijke)</a>
                                                        <div class="submenu-content">
                                                                <a href="/Tipos/HoTT/Rijke/index.html">üìã Resumen </a>
                                                                <a href="/Tipos/HoTT/Rijke/intro.html">üìñ Introducci√≥n </a>
                                                        </div>
                                                </div>
                                        </div>
                                </div>

                                <div class="dropdown-submenu">
                                        <a href="#" class="submenu-btn">üñ•Ô∏è Lean</a>
                                        <div class="submenu-content">
                                                <a href="/Tipos/Lean/index.html">üìã Resumen </a>
                                        </div>
                                </div>
                        </div>
                </div>
        </div>
  </nav>
</div>
<div id="content" class="content">
<h1 class="title">Regresi√≥n lineal</h1>
<div id="outline-container-orgb485bfc" class="outline-2">
<h2 id="orgb485bfc">Regresi√≥n lineal</h2>
<div class="outline-text-2" id="text-orgb485bfc">
<p>
Para hacer m√°s interesante nuestro ejemplo de las casas, consideremos un conjunto de datos
m√°s rico, en el que tambi√©n conocemos el n√∫mero de cuartos en cada casa:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-right">√Årea \((pies^2)\)</td>
<td class="org-right">#cuartos</td>
<td class="org-right">Precio (1000$)</td>
</tr>

<tr>
<td class="org-right">2104</td>
<td class="org-right">3</td>
<td class="org-right">400</td>
</tr>

<tr>
<td class="org-right">1600</td>
<td class="org-right">3</td>
<td class="org-right">330</td>
</tr>

<tr>
<td class="org-right">2400</td>
<td class="org-right">3</td>
<td class="org-right">369</td>
</tr>

<tr>
<td class="org-right">1416</td>
<td class="org-right">2</td>
<td class="org-right">232</td>
</tr>

<tr>
<td class="org-right">3000</td>
<td class="org-right">4</td>
<td class="org-right">540</td>
</tr>

<tr>
<td class="org-right">\(\vdots\)</td>
<td class="org-right">\(\vdots\)</td>
<td class="org-right">\(\vdots\)</td>
</tr>
</tbody>
</table>

<p>
Aqu√≠, las \(x\)'s son vectores de dos dimensiones en \(\mathbb{R}^2\). Por ejemplo, \(x_1^{(i)}\)
es el √°rea de la $i$-√©sima casa en el conjunto de entrenamiento y \(x_2^{(i)}\) el n√∫mero 
de cuartos.
</p>

<p>
Para realizar el aprendizaje supervisado, debemos decidir c√≥mo vamos a representar las 
funciones/hip√≥tesis \(h\) en una computadora. Una elecci√≥n inicial, digamos donde 
aproximamos a \(y\) como una funci√≥n lineal de \(x\):
\[h_\theta = \theta_0 + \theta_1 x + \theta_2 x_2.\]
</p>

<p>
Aqu√≠, las \(\theta_i\)'s son los <b>par√°metros</b> (tambi√©n llamados <b>pesos</b>) que parametrizan
el espacio de funciones lineal que mapean a \(\mathcal{X}\) en \(\mathcal{Y}\).
Cuando no haya peligro de confusi√≥n, dejamos de lado el sub-√≠ndice \(\theta\) y escribimos 
simplemente \(h(x)\). Para simplificar la notaci√≥n, tomamos la convenci√≥n de hacer \(x_0 = 1\)
(este es el t√©rmino de <b>intercepto</b>), de modo que 
\[h(x) = \sum_{i=0}^{d} \theta_i x_i = \theta^T x, \]
donde en el lado derecho estamos viendo a \(\theta\) y a \(x\) como vectores y donde \(d\) es el
n√∫mero de variables de entrada (caracter√≠sticas) sin contar a \(x_0\).
</p>

<p>
Ahora, dado un conjunto de entrenamiento, ¬øc√≥mo escogemos (aprendemos) los par√°metros \(\theta\)?
Un m√©todo razonable ser√≠a hacer a \(h(x)\) cercano a \(y\), al menos para los ejemplos de entrenamiento
que tenemos. Para formalizar esto, definiremos una funci√≥n que mide, para cada valor de las 
\(\theta\)'s, qu√© tan cercanas est√°n las \(h(x^{(i)})\)'s a las \(y^{(i)}\)'s correspondientes. 
Definimos la <b>funci√≥n de costo</b>:
\[ J(\theta) = \frac{1}{2} \sum_{i = 1}^n \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2. \]
A esta funci√≥n de costo se le conoce como la funci√≥n de costo de los m√≠nimos cuadrados y da lugar 
al modelo de regresi√≥n usual de <b>m√≠nimos cuadrados</b>.
</p>
</div>
<div id="outline-container-org74b084c" class="outline-3">
<h3 id="org74b084c">El algoritmo LMS</h3>
<div class="outline-text-3" id="text-org74b084c">
<p>
Queremos escoger a \(\theta\) de tal manera que se minimice \(J(\theta)\). Para hacer esto, usaremos 
un algoritmo de b√∫squeda que empieza con una suposici√≥n inicial para \(\theta\) y luego cambia 
repetidamente su valor para hacer a \(J(\theta)\) cada vez m√°s peque√±a. Espec√≠ficamente, consideremos
el algoritmo del <b>descenso gradiente</b>, que comienza con una suposici√≥n inicial \(\theta\), y 
aplica repetidamente la siguiente actualizaci√≥n:
\[\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)\]
</p>

<p>
Aqu√≠, \(\alpha\) es llamada la <b>tasa de aprendizaje</b>. Este es un algoritmo muy natural que toma 
repetidamente un paso en la direcci√≥n de m√°ximo decrecimiento de \(J\).
</p>

<p>
Para implementar el algoritmo, debemos calcular cu√°l es el t√©rmino de derivada parcial en el lado 
derecho. Primero hagamos el caso en el que tenemos un solo ejemplo de entrenamiento \((x,y)\), de 
modo que podemos ignorar la definici√≥n de \(J\) como una suma. Tenemos:
</p>

\begin{align*}
  \frac{\partial}{\partial \theta_j} J(\theta) &= \frac{\partial}{\partial \theta_j} \frac{1}{2} (h_\theta)(x) - y)^2 \\
  &= 2 \cdot \frac{1}{2} (h_\theta(x) - y) \cdot \frac{\partial}{\partial \theta_j} (h_\theta(x)-y)\\
  &= (h_\theta(x) - y) \cdot \frac{\partial}{\partial \theta_j} \left( \sum_{i=0}^d \theta_i x_i - y \right) \\
  &= (h_\theta(x) - y)x_j
\end{align*}

<p>
De modo que, para un s√≥lo ejemplo de entrenamiento, esto nos da la regla de actualizaci√≥n:
\[\theta_j := \theta_j + \alpha (y^{(i))} - h_\theta (x^{(i))})) x_j^{(i)}.\]
A esta regla se le conoce como la regla de actualizaci√≥n <b>LMS</b> (Least Mean Squares) o la regla 
de aprendizaje <b>Widrow-Hoff</b>. Esta regla tiene varias propiedades que parecen naturales e
intuitivas. Por ejemplo, la magnitud de la actualizaci√≥n es proporcional al t√©rmino de 
<b>error</b> \((y^{(i))} - h_\theta(x^{(i)})\); por lo tanto, si encontramos un ejemplo de 
entrenamiento en el que nuestra predicci√≥n es casi exacta, el par√°metro se actualiza por 
una cantidad peque√±a y, por el contrario, si el error es grande, se actualiza por una 
cantidad mayor.
</p>

<p>
Hemos derivado la regla <b>LMS</b> para cuando hay s√≥lo un ejemplo de entrenamiento. Hay dos
maneras de modificar este m√©todo para un conjunto de entrenamiento de m√°s de un ejemplo.
El primero es reemplazarlo por el siguiente m√©todo:
</p>
<ul class="org-ul">
<li>Repetir hasta que haya convergencia
\[
        \theta_j := \theta_j + \alpha \sum_{i=1}^n (y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}, \text{ para cada }j
  \]
Agrupando las coordenadas en un vector de actualizaci√≥n \( \theta \), podemos escribir
esto de manera m√°s sucinta:
\[
        \theta := \theta + \alpha \sum_{i=1}^n (y^{(i)} - h_\theta (x^{(i)}))x^{(i)}
  \]</li>
</ul>

<p>
Se puede verificar que la cantidad en la suma en la regla de actualizaci√≥n de arriba no
es m√°s que \( \partial J(\theta)/ \partial \theta_j \). De modo que esto es simplemente
descenso gradiente sobre la funci√≥n de costo original \( J \).
Este m√©todo se fija en cada ejemplo de todo el conjunto de entrenamiento en cada paso, y
es llamado <b>descenso gradiente en grupos</b>. Notemos que, mientras que el descenso gradiente
puede ser susceptible a m√≠nimos locales, el problema de optimizaci√≥n que hemos planteado
aqu√≠ para regresi√≥n lineal tiene √≥ptimos globales y ninguno otro local, de modo que el
descenso gradiente siempre converge (asumiendo que la tasa de aprendizaje \(\alpha\)
no es muy grande) al m√≠nimo global.
</p>

<p>
Hay otra alternativa al descenso gradiente en grupos que tambi√©n funciona muy bien.
Consideremos el siguiente algoritmo:
</p>
</div>
</div>
</div>
<div id="outline-container-orgfb5fa9a" class="outline-2">
<h2 id="orgfb5fa9a">Las ecuaciones normales</h2>
<div class="outline-text-2" id="text-orgfb5fa9a">
<p>
El descenso gradiente nos da una forma de minimizar a \(J\). Veamos una segunda manera de
hacer esto, esta vez haciendo la minimizaci√≥n de forma expl√≠cita, sin recurrir a un
un algoritmo iterativo. En este m√©todo, minimizaremos \(J\) tomando expl√≠citamente
las derivadas con respecto de las \(\theta_j\)'s, e igual√°ndolas a cero. Para permitir
que escribamos esto sin tener que escribir un mont√≥n de √°lgebra, introduzcamos un poco de
notaci√≥n.
</p>
</div>
<div id="outline-container-org491eb1e" class="outline-3">
<h3 id="org491eb1e">Derivadas matriciales</h3>
<div class="outline-text-3" id="text-org491eb1e">
<p>
Para una funci√≥n \( f: \mathbb{R}^{n \times d} \to \mathbb{R} \) que mapea las matrices de
\(n \times d\) a \(\mathbb{R}\)
</p>
</div>
</div>
</div>
</div>
</body>
</html>
